{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk34si2FxbCg"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyF7Io4yxJQy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import metrics\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import utils\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j70j3BSvxfIW"
      },
      "source": [
        "# Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3DjIgopxgya"
      },
      "outputs": [],
      "source": [
        "class ICTTestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, test_examples, demos, tokenizer, args):\n",
        "        self.examples = []\n",
        "        for test_example in test_examples:\n",
        "            input_text = utils.create_input_text(demos, test_example[0], 'label:', '. ')\n",
        "            if len(tokenizer(input_text)['input_ids']) <= args.max_input_len:\n",
        "                self.examples.append([input_text, test_example[1]])\n",
        "\n",
        "        tokenized_input = tokenizer([example[0] for example in self.examples], padding=True, truncation=True, max_length=args.max_input_len)\n",
        "        self.input_ids = tokenized_input['input_ids']\n",
        "        self.attention_mask = tokenized_input['attention_mask']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.LongTensor(self.input_ids[idx]), torch.LongTensor(self.attention_mask[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxAivTalxoDI"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voIviMTuxIaz"
      },
      "outputs": [],
      "source": [
        "class TrainingArgs:\n",
        "  def __init__(self):\n",
        "      self.test_data = 'data/test-train_classification_test_classification.json'\n",
        "      self.output_dir = 'output'\n",
        "      self.checkpoint_path = 'output/train_classification_test_classification_9.pt'\n",
        "      self.t5_model = 't5-base'\n",
        "      self.batch_size = 8\n",
        "      self.k = 8\n",
        "      self.n_prompt = 4\n",
        "      self.max_input_len = 1024\n",
        "\n",
        "args = TrainingArgs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kLMvIIcx7RZ"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bCpyTG1ysbz"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(level=logging.INFO)\n",
        "\n",
        "logFileFormatter = logging.Formatter(\n",
        "    fmt='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "    datefmt='%m/%d/%Y %H:%M:%S',\n",
        ")\n",
        "fileHandler = logging.FileHandler(filename=os.path.join(args.output_dir, 'log.txt'))\n",
        "fileHandler.setFormatter(logFileFormatter)\n",
        "fileHandler.setLevel(level=logging.INFO)\n",
        "\n",
        "logger.addHandler(fileHandler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFESTX1CxLwV"
      },
      "outputs": [],
      "source": [
        "utils.random_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(args.t5_model, model_max_length=1024)\n",
        "model = T5ForConditionalGeneration.from_pretrained(args.t5_model).to(device)\n",
        "model.load_state_dict(torch.load(args.checkpoint_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "test_raw_data = json.load(open(args.test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3y7zreLyFce"
      },
      "source": [
        "# Generate prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSOo2ahuxRcb"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for task in test_raw_data:\n",
        "    new_row = [task['task_name']]\n",
        "    for _ in range(args.n_prompt):\n",
        "        demos = utils.sample_demos(task['dev_examples'], args.k, utils.n_label(task['task_name']))\n",
        "        demo_text = utils.create_input_text(demos, None, 'label:', '. ')\n",
        "        demo_text_len = len(tokenizer(demo_text)['input_ids'])\n",
        "        new_row.extend([demos, demo_text_len])\n",
        "    data.append(new_row)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE3mn5SmyiwI"
      },
      "outputs": [],
      "source": [
        "EXCLUDED_TASKS = ['yelp_polarity', 'tab_fact']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73L8iPT3yVHs"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoy0_GFhyU2g"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "\n",
        "for task in test_raw_data:\n",
        "    if task['task_name'] in EXCLUDED_TASKS:\n",
        "        continue\n",
        "    logger.info('Evaluating on task {}...'.format(task['task_name']))\n",
        "\n",
        "    # Prepare data\n",
        "    demos = df[df[0] == task['task_name']].iloc[0][1]\n",
        "    test_dataset = ICTTestDataset(task['test_examples'], demos, tokenizer, args)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
        "\n",
        "    # Predict\n",
        "    predictions = []\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids, attention_mask = batch\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids.to(device),\n",
        "            attention_mask=attention_mask.to(device),\n",
        "            do_sample=False)\n",
        "        pred = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        predictions.extend(pred)\n",
        "\n",
        "    test_performance = metrics.evaluate(predictions, test_dataset.examples, metrics.METRICS[task['task_name']])\n",
        "    logger.info('Test score: {}; Metric: {}'.format(test_performance, metrics.METRICS[task['task_name']]))\n",
        "\n",
        "    result.append([task['task_name'], predictions, test_performance])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
