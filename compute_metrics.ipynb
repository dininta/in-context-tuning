{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "TEST_FILE = \"data/test-random.tsv\"\n",
    "PREDICTION_FILE = \"output/predictions/random_base1.1\"\n",
    "\n",
    "# Read test file\n",
    "df = pd.read_csv(TEST_FILE, sep=\"\\t\", header=None)\n",
    "n_col = df.shape[1]\n",
    "all_targets = df[range(4, n_col)].apply(\n",
    "    lambda row: row[~row.isnull()].tolist(), axis=1)\n",
    "df = df[range(4)]\n",
    "df.columns = [\"task_name\", \"task_prefix\", \"input\", \"target\"]\n",
    "df[\"all_targets\"] = all_targets\n",
    "\n",
    "# Read prediction file\n",
    "with open(PREDICTION_FILE) as fin:\n",
    "    predictions = fin.readlines()\n",
    "predictions = [x.replace(\"\\n\", \"\") for x in predictions]\n",
    "df[\"prediction\"] = predictions\n",
    "\n",
    "df = df.groupby([\"task_name\", \"task_prefix\"]).agg({\n",
    "    'all_targets': lambda x: list(x),\n",
    "    'prediction': lambda x: list(x)}).reset_index()\n",
    "\n",
    "def compute_metrics(row):\n",
    "    return metrics.evaluate(row.prediction, row.all_targets, metrics.METRICS[row.task_name], row.task_name)\n",
    "\n",
    "df[\"score\"] = df.apply(compute_metrics, axis=1)\n",
    "df.groupby([\"task_name\"]).agg({'score': [\"mean\", \"var\"]})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
