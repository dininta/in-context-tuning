{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pandas as pd\n",
    "import seqio\n",
    "import t5\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tsv_path = {\n",
    "    \"train\": \"data/train-train_clf_test_clf.tsv\",\n",
    "    \"test\": \"data/test-train_clf_test_clf.tsv\"\n",
    "}\n",
    "\n",
    "def read_prompt_dict(filename: str) -> dict:\n",
    "    result = {}\n",
    "    df = pd.read_csv(filename, header=None, sep=\"\\t\", names=[\"task_name\", \"task_prefix\", \"prompt\", \"prompt_len\", \"io_sep\"])\n",
    "    for _, row in df.iterrows():\n",
    "        result[row.task_prefix] = (row.prompt, row.io_sep)\n",
    "    return result\n",
    "\n",
    "PROMPT_DICT = read_prompt_dict(\"data/prompt/prompt.tsv\")\n",
    "\n",
    "def dataset_fn(split, shuffle_files=False):\n",
    "    del shuffle_files  # We only have one file for each split.\n",
    "\n",
    "    df = pd.read_csv(tsv_path[split], header=None, sep=\"\\t\")\n",
    "    df = df[range(4)]  # Only take the first 4 columns.\n",
    "    df.columns = [\"task_name\", \"task_prefix\", \"input\", \"target\"]\n",
    "    lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt_prefix, io_sep = PROMPT_DICT[row.task_prefix]\n",
    "        input_text = prompt_prefix + \" \" + row.input + \" \" + io_sep\n",
    "        lines.append(input_text + \"\\t\" + str(row.target))\n",
    "    ds = tf.data.Dataset.from_tensor_slices(lines)\n",
    "    # Split each \"<input>\\t<target>\" example into (input, target) tuple.\n",
    "    ds = ds.map(\n",
    "        functools.partial(\n",
    "            tf.io.decode_csv, record_defaults=[\"\", \"\"], field_delim=\"\\t\", use_quote_delim=False\n",
    "        ), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
    "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
    "    return ds\n",
    "\n",
    "def preprocessor_fn(ds):\n",
    "    def normalize_text(text):\n",
    "        \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "        text = tf.strings.lower(text)\n",
    "        text = tf.strings.regex_replace(text, \"'(.*)'\", r\"\\1\")\n",
    "        return text\n",
    "    def to_inputs_and_targets(ex):\n",
    "        \"\"\"Map {\"input\": ..., \"target\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
    "        return {\n",
    "            \"inputs\": normalize_text(ex[\"input\"]),\n",
    "            \"targets\": normalize_text(ex[\"target\"])\n",
    "        }\n",
    "    return ds.map(to_inputs_and_targets, \n",
    "                  num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset_fn(\"train\")\n",
    "train_ds = preprocessor_fn(train_ds)\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(vocabulary=t5.data.get_default_vocabulary(), add_eos=True),\n",
    "    \"targets\": seqio.Feature(vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "tokenized_train_ds = seqio.preprocessors.tokenize(\n",
    "    train_ds,\n",
    "    DEFAULT_OUTPUT_FEATURES,\n",
    "    copy_pretokenized=True, with_eos=True)\n",
    "\n",
    "for x in tokenized_train_ds.take(25663):\n",
    "    if x[\"inputs\"].shape[0] > 1024:\n",
    "        print(x[\"inputs\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = t5.data.get_default_vocabulary()\n",
    "vocab.encode_tf(tf.constant(\"the quick brown fox\")).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"data/train-train_non_mrc_qa_test_mrc.tsv\") as fin:\n",
    "    lines = fin.readlines()\n",
    "text = ''.join(lines)\n",
    "text = re.sub('\\s+','',text)\n",
    "text = re.sub('[ -~]', '', text)\n",
    "\n",
    "set1 = set()\n",
    "for c in text:\n",
    "    set1.add(c)\n",
    "weirdtext = ''.join([a for a in set1])\n",
    "print(weirdtext)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
