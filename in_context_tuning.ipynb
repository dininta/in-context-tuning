{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune T5 with in-context objective.\n",
    "\n",
    "This notebook is adapted from the original T5 [repo](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installing dependencies...\")\n",
    "%tensorflow_version 2.x\n",
    "!pip install seqio==0.0.12\n",
    "!pip install -q t5\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import t5\n",
    "import t5.models\n",
    "import seqio\n",
    "\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "BASE_DIR = \"gs://in-context-tuning\"\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "\n",
    "# Set credentials for GCS reading/writing from Colab and TPU.\n",
    "print(\"Setting up GCS access...\")\n",
    "os.environ['USE_AUTH_EPHEM'] = '0'\n",
    "TPU_TOPOLOGY = \"v2-8\"\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "TPU_ADDRESS = tpu.get_master()\n",
    "print('Running on TPU:', TPU_ADDRESS)\n",
    "tf.enable_eager_execution()\n",
    "tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Improve logging.\n",
    "from contextlib import contextmanager\n",
    "import logging as py_logging\n",
    "tf.get_logger().propagate = False\n",
    "py_logging.root.setLevel('INFO')\n",
    "\n",
    "@contextmanager\n",
    "def tf_verbosity_level(level):\n",
    "  og_level = tf.logging.get_verbosity()\n",
    "  tf.logging.set_verbosity(level)\n",
    "  yield\n",
    "  tf.logging.set_verbosity(og_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import functools\n",
    "import pandas as pd\n",
    "import t5\n",
    "import seqio\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "train_tsv_file = \"train-random.tsv\" #@param { type: \"string\" }\n",
    "validation_tsv_file = \"dev-random.tsv\" #@param { type: \"string\" }\n",
    "test_tsv_file = \"test-random.tsv\" #@param { type: \"string\" }\n",
    "num_examples_json_file = \"counts-random.json\" #@param { type: \"string\" }\n",
    "MAX_TARGET_LEN = 32  #@param { type: \"integer\" }\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(vocabulary=t5.data.get_default_vocabulary(), add_eos=True),\n",
    "    \"targets\": seqio.Feature(vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "TSV_PATH = {\n",
    "    \"train\": os.path.join(DATA_DIR, train_tsv_file),\n",
    "    \"validation\": os.path.join(DATA_DIR, validation_tsv_file) if validation_tsv_file else None,\n",
    "    \"test\": os.path.join(DATA_DIR, test_tsv_file)\n",
    "}\n",
    "num_examples = json.load(tf.io.gfile.GFile(os.path.join(DATA_DIR, num_examples_json_file)))\n",
    "\n",
    "def read_prompt_dict(filename: str) -> dict:\n",
    "    result = {}\n",
    "    df = pd.read_csv(\n",
    "        tf.io.gfile.GFile(filename), header=None, sep=\"\\t\",\n",
    "        names=[\"task_name\", \"task_prefix\", \"prompt\", \"prompt_len\", \"io_sep\"])\n",
    "    for _, row in df.iterrows():\n",
    "        result[row.task_prefix] = (row.prompt, row.io_sep)\n",
    "    return result\n",
    "\n",
    "PROMPT_DICT = read_prompt_dict(os.path.join(DATA_DIR, \"prompt/prompt.tsv\"))\n",
    "\n",
    "\n",
    "def preprocessor_fn(ds):\n",
    "    def normalize_text(text):\n",
    "        \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
    "        text = tf.strings.lower(text)\n",
    "        text = tf.strings.regex_replace(text, \"'(.*)'\", r\"\\1\")\n",
    "        return text\n",
    "\n",
    "    def to_inputs_and_targets(ex):\n",
    "        \"\"\"Map {\"input\": ..., \"target\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n",
    "        return {\n",
    "            \"inputs\": normalize_text(ex[\"input\"]),\n",
    "            \"targets\": normalize_text(ex[\"target\"])\n",
    "        }\n",
    "\n",
    "    return ds.map(to_inputs_and_targets, \n",
    "                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "def dataset_fn(split, shuffle_files=False):\n",
    "    del shuffle_files  # We only have one file for each split.\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        tf.io.gfile.GFile(TSV_PATH[split]), header=None, sep=\"\\t\")\n",
    "    df = df[range(4)]  # Only take the first 4 columns.\n",
    "    df.columns = [\"task_name\", \"task_prefix\", \"input\", \"target\"]\n",
    "    lines = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt_prefix, io_sep = PROMPT_DICT[row.task_prefix]\n",
    "        input_text = prompt_prefix + \" \" + row.input + \" \" + io_sep\n",
    "        lines.append(input_text + \"\\t\" + str(row.target))\n",
    "    ds = tf.data.Dataset.from_tensor_slices(lines)\n",
    "    # Split each \"<input>\\t<target>\" example into (input, target) tuple.\n",
    "    ds = ds.map(\n",
    "        functools.partial(\n",
    "            tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
    "            field_delim=\"\\t\", use_quote_delim=False\n",
    "        ), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
    "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
    "    return ds\n",
    "\n",
    "\n",
    "splits = [\"train\", \"validation\", \"test\"] if validation_tsv_file else [\"train\", \"test\"]\n",
    "seqio.TaskRegistry.add(\n",
    "    \"in_context_tuning\",\n",
    "    source=seqio.FunctionDataSource(\n",
    "        dataset_fn=dataset_fn,\n",
    "        splits=splits,\n",
    "        num_input_examples=num_examples),\n",
    "    preprocessors=[\n",
    "        preprocessor_fn,\n",
    "        seqio.preprocessors.tokenize_and_append_eos,\n",
    "    ],\n",
    "    postprocess_fn=t5.data.postprocessors.lower_text,\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"t5.1.1.large\" #@param[\"small\", \"base\", \"large\", \"t5.1.1.small\", \"t5.1.1.base\", \"t5.1.1.large\"]\n",
    "MODEL_DIR_NAME = \"train_random_large1.1\" #@param { type: \"string\" }\n",
    "PRETRAINED_DIR = os.path.join(\"gs://t5-data/pretrained_models\", MODEL_SIZE)\n",
    "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_DIR_NAME)\n",
    "\n",
    "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
    "    \"small\": (1, 256, 16),\n",
    "    \"t5.1.1.small\": (1, 256, 16),\n",
    "    \"base\": (2, 128, 8),\n",
    "    \"t5.1.1.base\": (2, 128, 8),\n",
    "    \"large\": (8, 64, 4),\n",
    "    \"t5.1.1.large\": (8, 64, 4)}[MODEL_SIZE]\n",
    "\n",
    "tf.io.gfile.makedirs(MODEL_DIR)\n",
    "\n",
    "model = t5.models.MtfModel(\n",
    "    model_dir=MODEL_DIR,\n",
    "    tpu=TPU_ADDRESS,\n",
    "    tpu_topology=TPU_TOPOLOGY,\n",
    "    model_parallelism=model_parallelism,\n",
    "    batch_size=train_batch_size,\n",
    "    sequence_length={\"inputs\": 1024, \"targets\": MAX_TARGET_LEN},\n",
    "    learning_rate_schedule=0.003,\n",
    "    save_checkpoints_steps=5000,\n",
    "    keep_checkpoint_max=keep_checkpoint_max,\n",
    "    iterations_per_loop=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ON_CLOUD:\n",
    "  %reload_ext tensorboard\n",
    "%tensorboard --logdir=\"$MODEL_DIR\" --port=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_STEPS = 30000 #@param {type: \"integer\"}\n",
    "\n",
    "model.finetune(\n",
    "    mixture_or_task_name=\"in_context_tuning\",\n",
    "    pretrained_model_dir=PRETRAINED_DIR,\n",
    "    finetune_steps=FINETUNE_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_STEPS = 1030000 #@param {type: \"integer\"}\n",
    "\n",
    "model.eval(\n",
    "    mixture_or_task_name=\"in_context_tuning\",\n",
    "    checkpoint_steps=CHECKPOINT_STEPS,\n",
    "    split=\"test\",\n",
    "    compute_sequence_length=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
