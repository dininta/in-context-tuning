{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "TASKS_SPLITS = \"data/custom_tasks_splits/train_classification_test_classification.json\"\n",
    "OUTPUT_FILE = {\n",
    "    \"train\": \"data/train-train_classification_test_classification.tsv\",\n",
    "    \"test\": \"data/test-train_classification_test_classification.tsv\"\n",
    "}\n",
    "COUNT_OUTPUT_FILE = \"data/counts-train_classification_test_classification.json\"\n",
    "DATA_PATH = \"data/crossfit\"\n",
    "INPUT_MAX_LEN = 1024\n",
    "\n",
    "\n",
    "def read_prompt_dict(filename: str) -> dict:\n",
    "    result = {}\n",
    "    df = pd.read_csv(filename, header=None, sep=\"\\t\", names=[\"task_name\", \"task_prefix\", \"prompt\", \"prompt_len\", \"io_sep\"])\n",
    "    for _, row in df.iterrows():\n",
    "        result[row.task_prefix] = row.prompt_len\n",
    "    return result\n",
    "\n",
    "PROMPT_DICT = read_prompt_dict(\"data/prompt/prompt.tsv\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=INPUT_MAX_LEN)\n",
    "\n",
    "def get_task_prefixes(data_path: str, task_name: str) -> list:\n",
    "    \"\"\"Returns all task prefixes (e.g., adversarialqa_32_13) of a task.\"\"\"\n",
    "    files = sorted(os.listdir(os.path.join(data_path, task_name)))\n",
    "    prefixes = []\n",
    "    for filename in files:\n",
    "        if not filename.endswith(\".tsv\"):\n",
    "            continue\n",
    "        prefix = \"_\".join(filename.split(\"_\")[:-1])\n",
    "        if prefix not in prefixes:\n",
    "            prefixes.append(prefix)\n",
    "    return prefixes\n",
    "\n",
    "def get_tasks_list(filename, split_name):\n",
    "    with open(filename, \"r\") as fin:\n",
    "        split_dict = json.load(fin)\n",
    "    return split_dict[split_name]\n",
    "\n",
    "def is_input_valid(task_prefix: str, input_text: str) -> bool:\n",
    "    max_allowed = INPUT_MAX_LEN - PROMPT_DICT[task_prefix]\n",
    "    n_tokens = len(tokenizer(input_text)[\"input_ids\"])\n",
    "    return n_tokens <= max_allowed\n",
    "\n",
    "\n",
    "n_examples = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "    task_names = get_tasks_list(TASKS_SPLITS, split)\n",
    "    data = []\n",
    "    for task_name in task_names:\n",
    "        prefixes = get_task_prefixes(DATA_PATH, task_name)\n",
    "        for prefix in prefixes:\n",
    "            with open(os.path.join(DATA_PATH, task_name, prefix + \"_\" + split + \".tsv\")) as fin:\n",
    "                lines = fin.readlines()\n",
    "            for line in lines:\n",
    "                d = line.strip().split(\"\\t\")\n",
    "                if is_input_valid(prefix, d[0]):\n",
    "                    data.append([task_name, prefix, d[0], random.choice(d[1:]), d[1:]])\n",
    "    n_examples[split] = len(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(OUTPUT_FILE[split], index=False, sep=\"\\t\", header=None)\n",
    "\n",
    "json.dump(n_examples, open(COUNT_OUTPUT_FILE, \"w\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
