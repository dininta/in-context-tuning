{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import metrics\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers.optimization import Adafactor\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args, tokenizer, split):\n",
    "        self.split = split\n",
    "        self.all_targets = []  # Only for test set. Empty otherwise.\n",
    "        examples = []\n",
    "        filename = args.task_prefix + \"_\" + split + \".tsv\" if split != \"test\" else args.test_filename\n",
    "\n",
    "        with open(os.path.join(args.data_dir, args.task_name, filename), encoding=\"utf-8\") as fin:\n",
    "            lines = fin.readlines()\n",
    "        for line in lines:\n",
    "            d = unidecode(line).strip().split(\"\\t\")\n",
    "            examples.append([d[0], random.choice(d[1:])])\n",
    "            if self.split == \"test\":\n",
    "                self.all_targets.append(d[1:])\n",
    "\n",
    "        tokenized_input = tokenizer([ex[0] for ex in examples], padding=True,\n",
    "                                    truncation=True, max_length=args.max_input_len)\n",
    "        self.input_ids = tokenized_input['input_ids']\n",
    "        self.attention_mask = tokenized_input['attention_mask']\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            tokenized_output = tokenizer([ex[1] for ex in examples], padding=True,\n",
    "                                         truncation=True, max_length=args.max_target_len)\n",
    "            # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "            label_ids = tokenized_output['input_ids']\n",
    "            for i in range(len(label_ids)):\n",
    "                label_ids[i] = [-100 if id == tokenizer.pad_token_id else id for id in label_ids[i]]\n",
    "            self.label_ids = label_ids\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"test\":\n",
    "            return (torch.LongTensor(self.input_ids[idx]),\n",
    "                    torch.LongTensor(self.attention_mask[idx]))\n",
    "        else:\n",
    "            return (torch.LongTensor(self.input_ids[idx]),\n",
    "                    torch.LongTensor(self.attention_mask[idx]),\n",
    "                    torch.LongTensor(self.label_ids[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)\n",
    "\n",
    "\n",
    "def train(args, logger, model, optimizer):\n",
    "    # Read training set\n",
    "    train_dataset = MyDataset(args, TOKENIZER, \"train\")\n",
    "    train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=args.batch_size)\n",
    "\n",
    "    # Read validation set\n",
    "    val_dataset = MyDataset(args, TOKENIZER, \"dev\")\n",
    "    val_loader = DataLoader(val_dataset, sampler=RandomSampler(val_dataset), batch_size=args.batch_size)\n",
    "\n",
    "    train_losses = []  # Train loss every eval_period step\n",
    "    val_losses = []  # Val loss every eval_period step\n",
    "    train_loss = []\n",
    "    step = 0\n",
    "    best_loss = float(\"inf\")\n",
    "    best_step = 0\n",
    "    stop_training = False\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(args.n_epochs):    \n",
    "        for batch in tqdm(train_loader, desc=\"Epoch {}\".format(epoch)):\n",
    "            input_ids, attention_mask, label_id = batch\n",
    "            loss = model(input_ids=input_ids.to(device),\n",
    "                         attention_mask=attention_mask.to(device),\n",
    "                         labels=label_id.to(device)).loss\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "\n",
    "            # Evaluate with validation set\n",
    "            if step % args.eval_period == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                for batch in val_loader:\n",
    "                    input_ids, attention_mask, label_id = batch\n",
    "                    loss = model(input_ids=input_ids.to(device),\n",
    "                                 attention_mask=attention_mask.to(device),\n",
    "                                 labels=label_id.to(device)).loss\n",
    "                    val_loss += loss.detach().item()\n",
    "                val_loss = val_loss / len(val_loader)\n",
    "                val_losses.append(val_loss)\n",
    "                train_losses.append(np.mean(train_loss))\n",
    "                logger.info('Step: {}; Train loss: {}; Val loss: {}'.format(step, np.mean(train_loss), val_loss))\n",
    "                train_loss = []\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_step = step\n",
    "                    logger.info('Found model with best loss at step {}'.format(step))\n",
    "                    torch.save(\n",
    "                        model.state_dict(),\n",
    "                        os.path.join(args.output_dir, args.finetuned_model_name.format(step)))\n",
    "                model.train()\n",
    "\n",
    "            #if step % args.save_period == 0:\n",
    "            #    torch.save(\n",
    "            #        model.state_dict(),\n",
    "            #        os.path.join(args.output_dir, args.finetuned_model_name.format(step)))\n",
    "\n",
    "            if step >= args.total_steps:\n",
    "                stop_training = True\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()        \n",
    "        if stop_training:\n",
    "            break\n",
    "\n",
    "    return best_step\n",
    "\n",
    "\n",
    "def evaluate(args, logger, model, best_step):\n",
    "    # Read test set\n",
    "    test_dataset = MyDataset(args, TOKENIZER, \"test\")\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    # Use the best model\n",
    "    model.load_state_dict(torch.load(\n",
    "        os.path.join(args.output_dir, args.finetuned_model_name.format(best_step)),\n",
    "        map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids, attention_mask = batch\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            max_length=args.max_target_len,\n",
    "            early_stopping=True)\n",
    "        predictions.extend(TOKENIZER.batch_decode(output, skip_special_tokens=True))\n",
    "\n",
    "    test_performance = metrics.evaluate(\n",
    "        predictions, test_dataset.all_targets, metrics.METRICS[args.task_name])\n",
    "    logger.info(\"Task: {}; Test score: {}; Metric: {}\".format(\n",
    "        args.task_prefix, test_performance, metrics.METRICS[args.task_name]))\n",
    "\n",
    "\n",
    "def plot_learning_curve(args, last_step, train_losses, val_losses):\n",
    "    li = list(zip(range(args.eval_period, last_step + 1, args.eval_period), train_losses))\n",
    "    plt.plot(*zip(*li), label=\"train\")\n",
    "    li = list(zip(range(args.eval_period, last_step + 1, args.eval_period), val_losses))\n",
    "    plt.plot(*zip(*li), label=\"val\")\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_task_prefixes_and_testfile(data_path: str, task_name: str) -> list:\n",
    "    \"\"\"Returns all task prefixes (e.g., adversarialqa_32_13) of a task.\"\"\"\n",
    "    files = sorted(os.listdir(os.path.join(data_path, task_name)))\n",
    "    prefixes = []\n",
    "    test_filename = None\n",
    "    for filename in files:\n",
    "        if not filename.endswith(\".tsv\"):\n",
    "            continue\n",
    "        if filename.endswith(\"test.tsv\"):\n",
    "            test_filename = filename\n",
    "        prefix = \"_\".join(filename.split(\"_\")[:-1])\n",
    "        if prefix not in prefixes:\n",
    "            prefixes.append(prefix)\n",
    "    return prefixes, test_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAMES = [\"paws copy\", \"glue-mrpc copy\"]\n",
    "\n",
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        self.task_name = \"\"  # Leave it empty, will be updated based on TASK_NAMES.\n",
    "        self.task_prefix = \"\"  # Leave it empty, will be updated based on TASK_NAMES.\n",
    "        self.test_filename = \"\"  # Leave it empty, will be updated based on TASK_NAMES.\n",
    "        self.data_dir = 'data/crossfit'\n",
    "        self.output_dir = 'output'\n",
    "        self.finetuned_model_name = \"\"  # Leave it empty, will be updated based on TASK_NAMES.\n",
    "        self.t5_model = 'google/t5-v1_1-base'\n",
    "        self.batch_size = 8\n",
    "        self.max_input_len = 1024\n",
    "        self.max_target_len = 8\n",
    "        self.n_epochs = 1\n",
    "        self.total_steps = 1000\n",
    "        self.eval_period = 1  # Evaluate with validation step every 100 steps\n",
    "        self.save_period = 200  # Save model every 200 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArgs()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TOKENIZER = T5Tokenizer.from_pretrained(args.t5_model, model_max_length=1024)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.INFO)\n",
    "\n",
    "logFileFormatter = logging.Formatter(\n",
    "    fmt='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    ")\n",
    "fileHandler = logging.FileHandler(filename=os.path.join(args.output_dir, 'log.txt'))\n",
    "fileHandler.setFormatter(logFileFormatter)\n",
    "fileHandler.setLevel(level=logging.INFO)\n",
    "\n",
    "logger.addHandler(fileHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name in TASK_NAMES:\n",
    "    prefixes, test_filename = get_task_prefixes_and_testfile(args.data_dir, task_name)\n",
    "    for prefix in prefixes:\n",
    "        # Update args\n",
    "        args.task_name = task_name\n",
    "        args.task_prefix = prefix\n",
    "        args.test_filename = test_filename\n",
    "        args.finetuned_model_name = prefix + \"_{}.pt\"\n",
    "\n",
    "        model = T5ForConditionalGeneration.from_pretrained(args.t5_model).to(device)\n",
    "        optimizer = Adafactor(model.parameters())\n",
    "        random_seed(0)\n",
    "\n",
    "        print(\"Start training for task {}\".format(args.task_prefix))\n",
    "        best_step = train(args, logger, model, optimizer)\n",
    "        print(\"Evaluating task {}\".format(args.task_prefix))\n",
    "        evaluate(args, logger, model, best_step)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
