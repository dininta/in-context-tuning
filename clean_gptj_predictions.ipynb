{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_prompt_dict(filename: str) -> dict:\n",
    "    result = {}\n",
    "    df = pd.read_csv(\n",
    "        filename, header=None, sep=\"\\t\",\n",
    "        names=[\"task_name\", \"task_prefix\", \"prompt\", \"prompt_len\", \"io_sep\"])\n",
    "    for _, row in df.iterrows():\n",
    "        result[row.task_prefix] = (row.prompt, row.io_sep)\n",
    "    return result\n",
    "\n",
    "PROMPT_DICT = read_prompt_dict(\"data/prompt/prompt.tsv\")\n",
    "\n",
    "TEST_FILE = \"data/test-train_clf_test_clf.tsv\"\n",
    "TASK_NAME = \"superglue-cb\"\n",
    "\n",
    "df = pd.read_csv(TEST_FILE, header=None, sep=\"\\t\")\n",
    "df = df[range(4)]  # Only take the first 4 columns.\n",
    "df.columns = [\"task_name\", \"task_prefix\", \"input\", \"target\"]\n",
    "df = df[:267].reset_index(drop=True)\n",
    "\n",
    "with open(os.path.join(\"data/predictions/gptj\", TASK_NAME)) as fin:\n",
    "    predictions = fin.readlines()\n",
    "predictions = [x.replace(\"\\n\", \"\") for x in predictions]\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    _, io_sep = PROMPT_DICT[row.task_prefix]    \n",
    "    predictions[i] = predictions[i].split(io_sep)[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"data/predictions/gptj\", TASK_NAME), \"w\") as f:\n",
    "    f.write(\"\\n\".join(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import metrics\n",
    "\n",
    "df[\"prediction\"] = predictions\n",
    "\n",
    "df = df.groupby([\"task_name\", \"task_prefix\"]).agg({'target': lambda x: list(x), 'prediction': lambda x: list(x)}).reset_index()\n",
    "\n",
    "def compute_f1(row):\n",
    "    return f1_score(row.target, row.prediction, average='macro', labels=metrics.LABELS[row.task_name])\n",
    "\n",
    "df[\"f1_score\"] = df.apply(compute_f1, axis=1)\n",
    "df.groupby([\"task_name\"]).agg({'f1_score': [\"mean\", \"var\"]})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "474cabef28ef5f39850ae0283db27a2b2b64f747313c5bcb210a7b2692fe8216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('thesis': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
